#!/usr/bin/python3.7
########!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback
import xlrd
import openpyxl

print('testing message1')
import re
import nltk
import langid
import emoji
import pandas as pd
from langdetect import detect
from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.corpus import brown
from nltk.corpus import stopwords



# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)

print(prefix)
print(input_path)
print(output_path)
print(model_path)
print(param_path)


print('*****')
print(training_path)
print('*****')
print('111')

# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            
        
        # Take the set of files and read them all into a single pandas dataframe
        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        if len(input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        
        print(input_files[0])
        data1 = pd.read_excel(input_files[0],'first_1k',engine='openpyxl')
        data1.rename(columns = {'label':'class'}, inplace = True)

        data2 = pd.read_excel(input_files[0],'second_expanded',engine='openpyxl')
        data = pd.concat([data1, data2])

        data.rename(columns = {'Unnamed: 0':'id'}, inplace = True)

        data3 = pd.read_excel(input_files[0],'listings_summary',engine='openpyxl')
        data3 = data3[['id', 'host_name']]
        data = pd.merge(data, data3, left_on='listing_id', right_on = 'id',how = 'left')

        corpus = list(data['review'])
        name = list(data['host_name'])
        label = list(data['class'])

        print('stage 1 done ')
        #option: replace host name
        for i in range(len(corpus)):
            corpus[i] = re.sub(name[i], 'host',str(corpus[i]))


        for i in range(len(corpus)):
          corpus[i] = re.sub(r'[0-9]+', '', str(corpus[i]))
          corpus[i] = re.sub(r"http\S+", "", str(corpus[i]))
          corpus[i] = emoji.demojize(str(corpus[i]), delimiters = ('',''))
        
        print('stage 2 done ')
        
        
        #use langid package here
        del_list_li = []
        for i in range(len(corpus)):
            if langid.classify(corpus[i])[0] != 'en':
                del_list_li.append(i)

        import numpy as np
        #delete non english sents
        corpus_en = np.delete(corpus, del_list_li).tolist()
        label_en = np.delete(label, del_list_li).tolist()

        #tokenization: use TweetTokenizer
        tknzr = TweetTokenizer()
        tokenized = []
        for sent in corpus_en:
            tosent = list(tknzr.tokenize(sent))
            #tosent = list(word_tokenize(sent))  #can use word_tokenize as well
            tokenized.append(tosent)

        #remove stopword: may change the content of stopwords
        nltk.download('stopwords')
        sw = stopwords.words('english')
        for _ in ['!', ',', '.','?','\r','\n','s','-','(',')',"'"]: #add content
            sw.append(_)
        sw.remove('with') #remove content
        filtered = []
        for sent in tokenized:
            filtered.append([w for w in sent if w.lower() not in sw])
    
        
        print('stage 3 done ')

        #lemmatization: firstly treat all words as 'v' to convert words like 'ate', 'met'
        nltk.download('wordnet')
        nltk.download('averaged_perceptron_tagger')

        wnl = WordNetLemmatizer()
        lemmated_ = []
        for filters in filtered:
            sent = []
            for word in filters:
                sent.append(wnl.lemmatize(word, 'v'))
            lemmated_.append(sent)

        #secondly use POS to convert other words
        def get_wordnet_pos(tag):
            if tag.startswith('J'):
                return wordnet.ADJ
            elif tag.startswith('V'):
                return wordnet.VERB
            elif tag.startswith('N'):
                return wordnet.NOUN
            elif tag.startswith('R'):
                return wordnet.ADV
            return None
           
            
        print('stage 4 done ')

        lemmated = []
        for lemmate in lemmated_:

            tagged = nltk.pos_tag(lemmate)

            lemmas_sent = []
            for tag in tagged:
                wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN
                lemmas_sent.append(wnl.lemmatize(tag[0], pos = wordnet_pos).lower()) #convert into lower case

            lemmated.append(lemmas_sent)

        #the output is lemmated result, if you want to change output, back to previous result like filtered
        #same to other code chunk
        output = {'review': pd.Series(lemmated), 'class': pd.Series(label_en)}
        output = pd.DataFrame(output)
        
        print(output.head())
        print('------')
        print(len(output['class']))
        print('------')
        
        # added fill na with 0 to handle the error
        
        output['class'] = output['class'].fillna(0)
        print('fixed')
        output['class'] = output['class'].astype(int)

        import collections, numpy

        print('stage 4 done ')

        pt_unique, pt_counts = numpy.unique(output['class'], return_counts=True)
        dict(zip(pt_unique, pt_counts))
        output=output
        for i in range(len(output.review)):
          if output['class'][i] == 1:
            output['class'][i] = '0'
          else:
            output['class'][i] = '1'

        import gensim.downloader as api
        from tqdm import tqdm

        # load pre-trained model
        word_emb_model = api.load('glove-wiki-gigaword-100')

        pretrained_w2v_size = 100

        word_set = set() 
        for sent in output.review:
            for word in sent:
                word_set.add(word)
        word_set.add('[PAD]')
        word_set.add('[UNKNOWN]')

        word_list = list(word_set) 
        word_list.sort()
        
        print('stage 5 done ')

        word_index = {}
        ind = 0
        for word in word_list:
            word_index[word] = ind
            ind += 1

        seq_length = 50

        def encode_and_add_padding(sentences, seq_length, word_index):
            sent_encoded = []
            for sent in sentences:
                temp_encoded = [word_index[word] if word in word_index else word_index['[UNKOWN]'] for word in sent]
                if len(temp_encoded) < seq_length:
                    temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))
                else:
                    temp_encoded = temp_encoded[:seq_length]
                sent_encoded.append(temp_encoded)
            return sent_encoded

        print('stage 5 done ')

        REVIEW_encoded = encode_and_add_padding(output.review, seq_length, word_index )

        import numpy as np
        emb_dim = word_emb_model.vector_size

        emb_table = []
        for i, word in enumerate(word_list):
            if word in word_emb_model:
                emb_table.append(word_emb_model[word])
            else:
                emb_table.append([0]*emb_dim)
        emb_table = np.array(emb_table)

        from sklearn.model_selection import train_test_split
        text_train,text_test,label_train,label_test = train_test_split(REVIEW_encoded,output['class'],test_size=0.25,random_state=1)

        vocab_size = len(word_list)
        unique_labels = np.unique(output['class'])
        n_class = len(unique_labels)
        n_hidden = 128
        learning_rate = 0.001
        total_epoch = 10

        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        import torch.optim as optim
        from sklearn.metrics import accuracy_score

        print('stage 6 done ')

        #You can enable GPU here (cuda); or just CPU
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        class Model(nn.Module):
            def __init__(self):
                super(Model, self).__init__()
                self.emb = nn.Embedding(vocab_size, emb_dim)
                self.emb.weight.data.copy_(torch.from_numpy(emb_table))
                self.emb.weight.requires_grad = False
                self.lstm = nn.LSTM(emb_dim, n_hidden, num_layers=2, batch_first =True, dropout=0.2)
                self.linear = nn.Linear(n_hidden,n_class)

            def forward(self, x):
                x = self.emb(x)
                x,_ = self.lstm(x)
                x = self.linear(x[:,-1,:])
                return x

        best_train_loss = 100
        best_epoch=0

        print('before to device')
        model = Model().to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)

        input_torch = torch.from_numpy(np.array(text_train)).to(device)
        target_torch = torch.from_numpy(np.array(label_train)).view(-1).to(device)

        # range(total_epoch)
        
        for epoch in range(1):  

            model.train()
            optimizer.zero_grad()
            outputs = model(input_torch) 
            loss = criterion(outputs, target_torch)
            loss.backward()
            optimizer.step()

            predicted = torch.argmax(outputs, -1)
            acc= accuracy_score(predicted.cpu().numpy(),target_torch.cpu().numpy())

            print('Epoch: %d, loss: %.5f, train_acc: %.2f' %(epoch + 1, loss.item(), acc))
            if loss.item() < best_train_loss:
              #torch.save(model.state_dict(), 'best-LSTM-model-parameters.pt') 
              with open(os.path.join(model_path, 'best-LSTM-model-parameters.pkl'), 'w') as out:
                pickle.dump(model.state_dict(), out)
            
            
              best_train_loss = loss.item()
              print('model updated')
              best_epoch=epoch+1   



        print('Finished Training')

# #         lstm_model=Model().to(device)
# #         lstm_model.load_state_dict(torch.load('best-LSTM-model-parameters.pt'))

#         with open(os.path.join(model_path, 'best-LSTM-model-parameters.pkl'), 'w') as out:
#             pickle.dump(Model(), out)
        
        # break line ---------
        
        # below are code for decision tree
#         print(input_files)
#         if len(input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
#         raw_data = [ pd.read_csv(file, header=None) for file in input_files ]
#         train_data = pd.concat(raw_data)

#         # labels are in the first column
#         train_y = train_data.ix[:,0]
#         train_X = train_data.ix[:,1:]

#         # Here we only support a single hyperparameter. Note that hyperparameters are always passed in as
#         # strings, so we need to do any necessary conversions.
#         max_leaf_nodes = trainingParams.get('max_leaf_nodes', None)
#         if max_leaf_nodes is not None:
#             max_leaf_nodes = int(max_leaf_nodes)

#         # Now use scikit-learn's decision tree classifier to train the model.
#         clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)
#         clf = clf.fit(train_X, train_y)

        # save the model
#         with open(os.path.join(model_path, 'decision-tree-model.pkl'), 'w') as out:
#             pickle.dump(clf, out)
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
